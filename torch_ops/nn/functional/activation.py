import torch
import torch.nn.functional as F

def threshold_bwd()

def threshold(input, threshold, value, inplace=False):
    return F.threshold(input, threshold, value, inplace=inplace), (threshold, value)

def threshold_(input, threshold, value):
    return F.threshold(input, threshold, value, inplace=True)

def relu():
    ...

def relu_():
    ...

def hardtanh():
    ...

def hardtanh_():
    ...

def hardswish():
    ...

def relu6():
    ...

def elu():
    ...

def elu_():
    ...

def selu():
    ...

def celu():
    ...

def leaky_relu():
    ...

def leaky_relu_():

    ...

def prelu():
    ...

def rrelu():
    ...

def glu():
    ...

def gelu():
    ...

def logsigmoid():
    ...

def hardshrink():
    ...

def tanhshrink():
    ...

def softsign():
    ...

def softplus():
    ...

def softmin():
    ...

def softmax():
    ...

def softshrink():
    ...

def gumbel_softmax():
    ...

def log_softmax():
    ...

def tanh():
    ...

def sigmoid():
    ...

def hardsigmoid():
    ...

def silu():
    ...

def mish():
    ...

def batch_norm():
    ...

def group_norm():
    ...

def instance_norm():
    ...

def layer_norm():
    ...

def local_response_norm():
    ...

def normalize():
    ...

